{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wildfire Size Prediction\n",
    "\n",
    "Predicting wildfire sizes for US states from 2011 to 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_processor import WildfireData\n",
    "from src.models import WildfirePredictor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import StackingRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from src.create_submission import create_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureNamePreserver(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.feature_names = None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        if hasattr(X, 'columns'):\n",
    "            self.feature_names = X.columns.tolist()\n",
    "            return X.to_numpy()\n",
    "        return X\n",
    "    \n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return self.feature_names\n",
    "\n",
    "def train_and_evaluate(data_processor, model_config):\n",
    "    predictor = WildfirePredictor(model_config)\n",
    "    \n",
    "    # Create pipeline\n",
    "    numerical_features = data_processor.X_train.select_dtypes(include=[np.number]).columns\n",
    "    categorical_features = data_processor.X_train.columns.drop(numerical_features)\n",
    "    \n",
    "    num_pipeline = Pipeline([\n",
    "        ('preserves_names', FeatureNamePreserver()),\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    cat_pipeline = Pipeline([\n",
    "        ('preserves_names', FeatureNamePreserver()),\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('encoder', OneHotEncoder(sparse_output=False))\n",
    "    ])\n",
    "\n",
    "    transform_pipeline = ColumnTransformer(\n",
    "        [\n",
    "            ('num', num_pipeline, numerical_features),\n",
    "            ('cat', cat_pipeline, categorical_features)\n",
    "        ],\n",
    "        verbose_feature_names_out=False\n",
    "    )\n",
    "\n",
    "    full_pipeline = Pipeline([\n",
    "        ('preprocessor', transform_pipeline),\n",
    "        ('model', predictor.model)\n",
    "    ])\n",
    "\n",
    "    # Train and evaluate\n",
    "    full_pipeline.fit(data_processor.X_train, data_processor.y_train)\n",
    "    \n",
    "    # Make predictions using full pipeline\n",
    "    train_score = calculate_custom_score(\n",
    "        full_pipeline.predict(data_processor.X_train),\n",
    "        data_processor.y_train\n",
    "    )\n",
    "    val_score = calculate_custom_score(\n",
    "        full_pipeline.predict(data_processor.X_val),\n",
    "        data_processor.y_val\n",
    "    )\n",
    "    \n",
    "    return full_pipeline, train_score, val_score, transform_pipeline.get_feature_names_out()\n",
    "\n",
    "def calculate_custom_score(y_pred, y_true):\n",
    "    # Add small epsilon to avoid log(0)\n",
    "    epsilon = 1e-10\n",
    "    y_pred = np.maximum(y_pred, epsilon)\n",
    "    y_true = np.maximum(y_true, epsilon)\n",
    "    \n",
    "    log_array = np.abs(np.log(y_pred/y_true))\n",
    "    constrained_log_array = np.minimum(log_array, 10)\n",
    "    sum_logs = np.sum(constrained_log_array)\n",
    "    \n",
    "    return -(1/len(y_true) * sum_logs)  # Negative because we want to maximize\n",
    "\n",
    "def create_stacking_model():\n",
    "    estimators = [\n",
    "        ('rf', RandomForestRegressor(n_estimators=100, random_state=42)),\n",
    "        ('gb', GradientBoostingRegressor(n_estimators=100, random_state=42)),\n",
    "        ('xgb', XGBRegressor(n_estimators=100, random_state=42))\n",
    "    ]\n",
    "    return StackingRegressor(\n",
    "        estimators=estimators,\n",
    "        final_estimator=Ridge(),\n",
    "        cv=5\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_config = {\n",
    "    'random_forest': {\n",
    "        'model_type': 'rf',\n",
    "        'model_params': {\n",
    "            'n_estimators': 200,\n",
    "            'max_depth': 15,\n",
    "            'min_samples_split': 5,\n",
    "            'random_state': 42\n",
    "        }\n",
    "    },\n",
    "    'gradient_boost': {\n",
    "        'model_type': 'gb',\n",
    "        'model_params': {\n",
    "            'n_estimators': 150,\n",
    "            'learning_rate': 0.05,\n",
    "            'max_depth': 8,\n",
    "            'subsample': 0.8,\n",
    "            'random_state': 42\n",
    "        }\n",
    "    },\n",
    "    'xgboost': {\n",
    "        'model_type': 'xgb',\n",
    "        'model_params': {\n",
    "            'n_estimators': 150,\n",
    "            'learning_rate': 0.05,\n",
    "            'max_depth': 8,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'random_state': 42\n",
    "        }\n",
    "    },\n",
    "    'lightgbm': {\n",
    "        'model_type': 'lgbm',\n",
    "        'model_params': {\n",
    "            'n_estimators': 150,\n",
    "            'learning_rate': 0.05,\n",
    "            'num_leaves': 32,\n",
    "            'subsample': 0.8,\n",
    "            'random_state': 42\n",
    "        }\n",
    "    },\n",
    "    'catboost': {\n",
    "        'model_type': 'catboost',\n",
    "        'model_params': {\n",
    "            'iterations': 150,\n",
    "            'learning_rate': 0.05,\n",
    "            'depth': 8,\n",
    "            'random_state': 42,\n",
    "            'verbose': False\n",
    "        }\n",
    "    },\n",
    "    'extra_trees': {\n",
    "        'model_type': 'ext',\n",
    "        'model_params': {\n",
    "            'n_estimators': 200,\n",
    "            'max_depth': 15,\n",
    "            'min_samples_split': 5,\n",
    "            'random_state': 42\n",
    "        }\n",
    "    },\n",
    "    'neural_net': {\n",
    "        'model_type': 'mlp',\n",
    "        'model_params': {\n",
    "            'hidden_layer_sizes': (100, 50, 25),\n",
    "            'activation': 'relu',\n",
    "            'max_iter': 1000,  # Increased from 500\n",
    "            'early_stopping': True,  # Added early stopping\n",
    "            'random_state': 42\n",
    "        }\n",
    "    },\n",
    "    'robust_regression': {\n",
    "        'model_type': 'huber',\n",
    "        'model_params': {\n",
    "            'epsilon': 1.35,\n",
    "            'max_iter': 200,\n",
    "            'alpha': 0.0001\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:  5266 samples\n",
      "Validation set:  1317 samples\n",
      "Test set:  5192 samples\n",
      "Done preparing data.\n",
      "\n",
      "Training random_forest...\n",
      "\n",
      "Training gradient_boost...\n",
      "\n",
      "Training xgboost...\n",
      "\n",
      "Training lightgbm...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000285 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 985\n",
      "[LightGBM] [Info] Number of data points in the train set: 5266, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 14052.692419\n",
      "\n",
      "Training catboost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pawel4388/ML/hackathon/venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/pawel4388/ML/hackathon/venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training extra_trees...\n",
      "\n",
      "Training neural_net...\n",
      "\n",
      "Training robust_regression...\n",
      "\n",
      "Model Comparison (Custom Evaluation Metric):\n",
      "\n",
      "random_forest:\n",
      "Train error: 1.8065\n",
      "Validation error: 2.1777\n",
      "(Lower error is better)\n",
      "\n",
      "gradient_boost:\n",
      "Train error: 2.6208\n",
      "Validation error: 2.9330\n",
      "(Lower error is better)\n",
      "\n",
      "xgboost:\n",
      "Train error: 2.7008\n",
      "Validation error: 3.0123\n",
      "(Lower error is better)\n",
      "\n",
      "lightgbm:\n",
      "Train error: 3.5316\n",
      "Validation error: 3.5695\n",
      "(Lower error is better)\n",
      "\n",
      "catboost:\n",
      "Train error: 3.3360\n",
      "Validation error: 3.3624\n",
      "(Lower error is better)\n",
      "\n",
      "extra_trees:\n",
      "Train error: 1.6544\n",
      "Validation error: 2.0570\n",
      "(Lower error is better)\n",
      "\n",
      "neural_net:\n",
      "Train error: 3.2169\n",
      "Validation error: 3.2023\n",
      "(Lower error is better)\n",
      "\n",
      "robust_regression:\n",
      "Train error: 3.1334\n",
      "Validation error: 2.9362\n",
      "(Lower error is better)\n",
      "\n",
      "Using best model for predictions...\n"
     ]
    }
   ],
   "source": [
    "# Initialize data processor\n",
    "data_processor = WildfireData(\n",
    "    fire_data_path='data/wildfire_sizes_before_2010.csv',\n",
    "    state_data_path='data/merged_state_data.csv',\n",
    "    weather_data_path='data/weather_monthly_state_aggregates.csv',\n",
    "    coordinates_path='data/state_coordinates.csv',\n",
    "    zero_submission_path='data/zero_submission.csv'\n",
    ")\n",
    "\n",
    "# Prepare data\n",
    "data_processor.prepare_data()\n",
    "\n",
    "X_test_original = data_processor.X_test.copy()\n",
    "\n",
    "data_processor.filter_features(['PRCP', 'EVAP', 'TMIN', 'TMAX', 'mean_elevation', \n",
    "                              'Land Area (sq mi)', 'Water Area (sq mi)', \n",
    "                              'Percentage of Federal Land', 'Urbanization Rate (%)', \n",
    "                              'latitude', 'longitude'])\n",
    "\n",
    "# Try all models\n",
    "results = {}\n",
    "best_model = None\n",
    "best_score = -np.inf\n",
    "feature_names = None\n",
    "\n",
    "for model_name, config in models_config.items():\n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    pipeline, train_score, val_score, feat_names = train_and_evaluate(data_processor, config)\n",
    "    feature_names = feat_names  # Save feature names for later use\n",
    "    \n",
    "    results[model_name] = {\n",
    "        'train_score': train_score,\n",
    "        'val_score': val_score,\n",
    "        'pipeline': pipeline\n",
    "    }\n",
    "    \n",
    "    if val_score > best_score:\n",
    "        best_score = val_score\n",
    "        best_model = pipeline\n",
    "\n",
    "# Print results with better formatting\n",
    "print(\"\\nModel Comparison (Custom Evaluation Metric):\")\n",
    "for model_name, scores in results.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"Train error: {-scores['train_score']:.4f}\")\n",
    "    print(f\"Validation error: {-scores['val_score']:.4f}\")\n",
    "    print(f\"(Lower error is better)\")\n",
    "\n",
    "# Use best model for predictions with numpy arrays\n",
    "print(f\"\\nUsing best model for predictions...\")\n",
    "X_test_transformed = best_model.named_steps['preprocessor'].transform(data_processor.X_test)\n",
    "y_pred = best_model.named_steps['model'].predict(X_test_transformed)\n",
    "\n",
    "# Save predictions\n",
    "assert len(X_test_original) == len(data_processor.X_test)\n",
    "create_submission(X_test_original, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

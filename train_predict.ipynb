{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wildfire Size Prediction\n",
    "\n",
    "Predicting wildfire sizes for US states from 2011 to 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import StackingRegressor, RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier, AdaBoostClassifier, RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, ExtraTreesRegressor\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.linear_model import ElasticNet, Lasso, Ridge, HuberRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel:\n",
    "    def __init__(self, config=None):\n",
    "        self.config = config or {}\n",
    "        self.model = self._create_model()\n",
    "    \n",
    "    def _create_model(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.model.fit(X, y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    def save(self, path):\n",
    "        joblib.dump(self.model, path)\n",
    "\n",
    "    def evaluate(self, X, y_true):\n",
    "        y_pred = self.predict(X)\n",
    "        N = len(y_true)\n",
    "        \n",
    "        # Add small epsilon to avoid log(0)\n",
    "        epsilon = 1e-10\n",
    "        y_pred = np.maximum(y_pred, epsilon)\n",
    "        y_true = np.maximum(y_true, epsilon)\n",
    "        \n",
    "        log_array = np.abs(np.log(y_pred/y_true))\n",
    "        constrained_log_array = np.minimum(log_array, 10)  # Changed min to minimum\n",
    "        sum_logs = np.sum(constrained_log_array)\n",
    "        \n",
    "        return -1 * (1/N * sum_logs)  # Negative because we want to maximize\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        instance = cls()\n",
    "        instance.model = joblib.load(path)\n",
    "        return instance\n",
    "\n",
    "\n",
    "class RandomForest(BaseModel):\n",
    "    def _create_model(self):\n",
    "        params = self.config.get('model_params', {})\n",
    "        return RandomForestClassifier(**params)\n",
    "\n",
    "\n",
    "class AdaBoost(BaseModel):\n",
    "    def _create_model(self):\n",
    "        params = self.config.get('model_params', {})\n",
    "        return AdaBoostClassifier(**params)\n",
    "\n",
    "\n",
    "class NeuralNetwork(BaseModel):\n",
    "    def _create_model(self):\n",
    "        params = self.config.get('model_params', {})\n",
    "        return MLPClassifier(**params)\n",
    "\n",
    "class ModelWrapper(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, estimator):\n",
    "        self.estimator = estimator\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Convert to numpy array if needed\n",
    "        X_arr = X.to_numpy() if hasattr(X, 'to_numpy') else X\n",
    "        self.estimator.fit(X_arr, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Convert to numpy array if needed\n",
    "        X_arr = X.to_numpy() if hasattr(X, 'to_numpy') else X\n",
    "        return self.estimator.predict(X_arr)\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {\"estimator\": self.estimator}\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        self.estimator = params[\"estimator\"]\n",
    "        return self\n",
    "\n",
    "class WildfirePredictor(BaseModel):\n",
    "    MODELS = {\n",
    "        'rf': RandomForestRegressor,\n",
    "        'gb': GradientBoostingRegressor,\n",
    "        'ada': AdaBoostRegressor,\n",
    "        'elastic': ElasticNet,\n",
    "        'xgb': XGBRegressor,\n",
    "        'lgbm': LGBMRegressor,  # Use direct LGBMRegressor\n",
    "        'catboost': CatBoostRegressor,\n",
    "        'ext': ExtraTreesRegressor,\n",
    "        'svr': SVR,\n",
    "        'mlp': MLPRegressor,\n",
    "        'huber': HuberRegressor,\n",
    "        'ridge': Ridge,\n",
    "        'lasso': Lasso\n",
    "    }\n",
    "\n",
    "    def _create_model(self):\n",
    "        params = self.config.get('model_params', {})\n",
    "        model_type = self.config.get('model_type', 'rf')\n",
    "        model_class = self.MODELS.get(model_type, RandomForestRegressor)\n",
    "        base_model = model_class(**params)\n",
    "        return ModelWrapper(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(predictions_df, output_path='submissions/submission.csv'):\n",
    "    \"\"\"\n",
    "    Create submission file from predictions DataFrame\n",
    "    \n",
    "    Args:\n",
    "        predictions_df: DataFrame with columns ['STATE', 'month', 'total_fire_size']\n",
    "        output_path: Path to save submission file\n",
    "    \"\"\"\n",
    "    # Ensure directory exists\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    # Load zero submission template\n",
    "    submission = pd.read_csv('data/zero_submission.csv')\n",
    "    \n",
    "    # Merge predictions with template\n",
    "    submission = submission[['ID', 'STATE', 'month']].merge(\n",
    "        predictions_df,\n",
    "        on=['STATE', 'month'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    submission = submission[['ID', 'STATE', 'month', 'total_fire_size']]\n",
    "\n",
    "    # Save submission file\n",
    "    submission.to_csv(output_path, index=False)\n",
    "    print(f\"Submission saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WildfireData:\n",
    "    def __init__(self, fire_data_path, state_data_path, weather_data_path, coordinates_path, zero_submission_path):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.data, self.X_train, self.X_val, self.X_test, self.y_train, self.y_val = None, None, None, None, None, None\n",
    "\n",
    "        self.load_data(fire_data_path, state_data_path, weather_data_path, coordinates_path, zero_submission_path)\n",
    "        self.target_col = 'total_fire_size'\n",
    "        self.features = self.data.columns.drop(self.target_col)\n",
    "\n",
    "    def filter_features(self, features):\n",
    "        self.X_train = self.X_train.filter(items=features)\n",
    "        self.X_val = self.X_val.filter(items=features)\n",
    "        self.X_test = self.X_test.filter(items=features)\n",
    "\n",
    "    def load_data(self, fire_data_path, state_data_path, weather_data_path, coordinates_path, zero_submission_path):\n",
    "        # Load datasets\n",
    "        fire_df = pd.read_csv(fire_data_path)\n",
    "        state_df = pd.read_csv(state_data_path)\n",
    "        weather_df = pd.read_csv(weather_data_path)\n",
    "        coordinates_df = pd.read_csv(coordinates_path)\n",
    "        self.zero_submission_path = zero_submission_path\n",
    "        \n",
    "        # Clean up coordinates data\n",
    "        coordinates_df = coordinates_df[['state&teritory', 'latitude', 'longitude']].rename(columns={'state&teritory': 'State'})\n",
    "        \n",
    "        # Remove any existing missing or zero rows from fire_df\n",
    "        fire_df = fire_df[fire_df['total_fire_size'] > 0]\n",
    "        \n",
    "        # Merge datasets with filled data\n",
    "        state_df = pd.merge(state_df, coordinates_df, on='State', how='left')\n",
    "        self.data = combine_data(states_df=state_df, weather_df=weather_df, target_df=fire_df)\n",
    "\n",
    "    \n",
    "\n",
    "    def prepare_data(self, val_size=0.2):\n",
    "        # Convert percentage strings to floats\n",
    "        self.data['Percentage of Federal Land'] = self.data['Percentage of Federal Land'].str.rstrip('%').astype(float) / 100\n",
    "        self.data['Urbanization Rate (%)'] = self.data['Urbanization Rate (%)'].astype(float) / 100\n",
    "        \n",
    "        # First create train set from existing data\n",
    "        train_set = self.data[self.data[self.target_col].notna()].copy()\n",
    "        print(f\"Training set size before split: {len(train_set)}\")\n",
    "        print(f\"Training target stats:\\n{train_set[self.target_col].describe()}\")\n",
    "        \n",
    "        # Create test set from submission template\n",
    "        zero_submission = pd.read_csv(self.zero_submission_path)\n",
    "        test_set = zero_submission[['STATE', 'month']].rename(columns={'STATE': 'State'})\n",
    "        test_set['year_month'] = test_set['month']  # Set year_month for merging\n",
    "        \n",
    "        # Add state features\n",
    "        state_columns = ['mean_elevation', 'Land Area (sq mi)', 'Water Area (sq mi)', \n",
    "                        'Percentage of Federal Land', 'Urbanization Rate (%)', \n",
    "                        'latitude', 'longitude']\n",
    "        state_data = self.data[['State'] + state_columns].drop_duplicates('State')\n",
    "        test_set = pd.merge(test_set, state_data, on='State', how='left')\n",
    "        \n",
    "        # Add weather features\n",
    "        weather_cols = ['PRCP', 'EVAP', 'TMIN', 'TMAX']\n",
    "        weather_data = self.data[['State', 'year_month'] + weather_cols].copy()\n",
    "        test_set = pd.merge(\n",
    "            test_set,\n",
    "            weather_data,\n",
    "            left_on=['State', 'year_month'],\n",
    "            right_on=['State', 'year_month'],\n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        # Add time-based features to both sets\n",
    "        for df in [train_set, test_set]:\n",
    "            df = self.add_month_feature(df)\n",
    "            df = self.add_season_feature(df)\n",
    "        \n",
    "        # Fill missing weather values in test set\n",
    "        for col in weather_cols:\n",
    "            # Calculate averages by state and season\n",
    "            avg_by_state_season = train_set.groupby(['State', 'season'])[col].mean()\n",
    "            avg_by_state = train_set.groupby('State')[col].mean()\n",
    "            \n",
    "            # Fill nulls with state-season average, then state average\n",
    "            for state in test_set['State'].unique():\n",
    "                state_mask = test_set['State'] == state\n",
    "                for season in test_set.loc[state_mask, 'season'].unique():\n",
    "                    mask = state_mask & (test_set['season'] == season)\n",
    "                    try:\n",
    "                        fill_val = avg_by_state_season.loc[(state, season)]\n",
    "                    except:\n",
    "                        fill_val = avg_by_state.loc[state]\n",
    "                    test_set.loc[mask & test_set[col].isna(), col] = fill_val\n",
    "        \n",
    "        # Create final splits\n",
    "        self.X_train = train_set.drop(columns=[self.target_col]).reset_index(drop=True)\n",
    "        self.y_train = train_set[self.target_col].values.ravel()\n",
    "        \n",
    "        self.X_test = test_set.reset_index(drop=True)\n",
    "        self.X_test = self.X_test.sort_values(['State', 'month']).reset_index(drop=True)\n",
    "        \n",
    "        # Split train & validation\n",
    "        self.X_train, self.X_val, self.y_train, self.y_val = split_data(\n",
    "            self.X_train, self.y_train, test_size=val_size, random=False\n",
    "        )\n",
    "        \n",
    "        print(\"\\nDataset splits:\")\n",
    "        print(f\"Training samples: {len(self.X_train)}\")\n",
    "        print(f\"Validation samples: {len(self.X_val)}\")\n",
    "        print(f\"Test samples: {len(self.X_test)}\")\n",
    "        print(\"\\nFeature statistics:\")\n",
    "        print(self.X_test.describe())\n",
    "        \n",
    "        # Verify no missing values\n",
    "        missing = self.X_test.isna().sum()\n",
    "        if missing.any():\n",
    "            print(\"\\nMissing values in test set:\")\n",
    "            print(missing[missing > 0])\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        # Scale features\n",
    "        self.scaler = self.scaler.fit(self.X_train)\n",
    "        self.X_train = self.scaler.transform(self.X_train)\n",
    "        self.X_test = self.scaler.transform(self.X_test)\n",
    "        self.X_val = self.scaler.transform(self.X_val)\n",
    "\n",
    "        # Set the type to np.float32\n",
    "        self.X_train = self.X_train.astype(np.float32)\n",
    "        self.X_train = self.X_val.astype(np.float32)\n",
    "        self.X_train = self.X_test.astype(np.float32)\n",
    "\n",
    "    def add_month_feature(self, data): \n",
    "        dates = pd.to_datetime(data['year_month'])\n",
    "        data['month_since_epoch'] = ((dates.dt.year - 1970) * 12 + dates.dt.month - 1).astype(int)\n",
    "        return data\n",
    "    \n",
    "    def add_season_feature(self, data): \n",
    "        if 'month_in_year' not in data.columns:\n",
    "            data['month_in_year'] = pd.to_datetime(data['year_month']).dt.month\n",
    "        \n",
    "        conditions = [\n",
    "            data['month_in_year'].isin([12, 1, 2]),\n",
    "            data['month_in_year'].isin([3, 4, 5]),\n",
    "            data['month_in_year'].isin([6, 7, 8]),\n",
    "            data['month_in_year'].isin([9, 10, 11])\n",
    "        ]\n",
    "        choices = ['winter', 'spring', 'summer', 'fall']\n",
    "        data['season'] = np.select(conditions, choices, default='unknown')\n",
    "        return data\n",
    "\n",
    "def split_data(X, y, test_size=0.2, random=False):\n",
    "    if random: \n",
    "        return train_test_split(X, y, test_size=test_size)\n",
    "    else: \n",
    "        return train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "    \n",
    "\n",
    "def combine_data(states_df, weather_df, target_df, how='left'):\n",
    "    target_df = target_df.rename(columns={'STATE': 'State', 'month': 'year_month'})\n",
    "    combined_df = pd.merge(weather_df, states_df, on='State', how='right')\n",
    "    combined_df = pd.merge(combined_df, target_df, on=['State', 'year_month'], how=how)\n",
    "    return combined_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureNamePreserver(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.feature_names = None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        if hasattr(X, 'columns'):\n",
    "            self.feature_names = X.columns.tolist()\n",
    "            return X.to_numpy()\n",
    "        return X\n",
    "    \n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return self.feature_names\n",
    "\n",
    "def train_and_evaluate(data_processor, model_config):\n",
    "    predictor = WildfirePredictor(model_config)\n",
    "    \n",
    "    # Create pipeline\n",
    "    numerical_features = data_processor.X_train.select_dtypes(include=[np.number]).columns\n",
    "    categorical_features = data_processor.X_train.columns.drop(numerical_features)\n",
    "    \n",
    "    num_pipeline = Pipeline([\n",
    "        ('preserves_names', FeatureNamePreserver()),\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    cat_pipeline = Pipeline([\n",
    "        ('preserves_names', FeatureNamePreserver()),\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('encoder', OneHotEncoder(sparse_output=False))\n",
    "    ])\n",
    "\n",
    "    transform_pipeline = ColumnTransformer(\n",
    "        [\n",
    "            ('num', num_pipeline, numerical_features),\n",
    "            ('cat', cat_pipeline, categorical_features)\n",
    "        ],\n",
    "        verbose_feature_names_out=False\n",
    "    )\n",
    "\n",
    "    # Fit transform pipeline first\n",
    "    X_train_transformed = transform_pipeline.fit_transform(data_processor.X_train)\n",
    "    X_val_transformed = transform_pipeline.transform(data_processor.X_val)\n",
    "    \n",
    "    # Train model separately\n",
    "    predictor.model.fit(X_train_transformed, data_processor.y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    train_score = calculate_custom_score(\n",
    "        predictor.model.predict(X_train_transformed),\n",
    "        data_processor.y_train\n",
    "    )\n",
    "    val_score = calculate_custom_score(\n",
    "        predictor.model.predict(X_val_transformed),\n",
    "        data_processor.y_val\n",
    "    )\n",
    "    \n",
    "    # Create final pipeline\n",
    "    full_pipeline = Pipeline([\n",
    "        ('preprocessor', transform_pipeline),\n",
    "        ('model', predictor.model)\n",
    "    ])\n",
    "    \n",
    "    return full_pipeline, train_score, val_score, transform_pipeline.get_feature_names_out()\n",
    "\n",
    "def calculate_custom_score(y_pred, y_true):\n",
    "    # Add small epsilon to avoid log(0)\n",
    "    epsilon = 1e-10\n",
    "    y_pred = np.maximum(y_pred, epsilon)\n",
    "    y_true = np.maximum(y_true, epsilon)\n",
    "    \n",
    "    log_array = np.abs(np.log(y_pred/y_true))\n",
    "    constrained_log_array = np.minimum(log_array, 10)\n",
    "    sum_logs = np.sum(constrained_log_array)\n",
    "    \n",
    "    return -(1/len(y_true) * sum_logs)  # Negative because we want to maximize\n",
    "\n",
    "def create_stacking_model():\n",
    "    estimators = [\n",
    "        ('rf', RandomForestRegressor(n_estimators=100, random_state=42)),\n",
    "        ('gb', GradientBoostingRegressor(n_estimators=100, random_state=42)),\n",
    "        ('xgb', XGBRegressor(n_estimators=100, random_state=42))\n",
    "    ]\n",
    "    return StackingRegressor(\n",
    "        estimators=estimators,\n",
    "        final_estimator=Ridge(),\n",
    "        cv=5\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_config = {\n",
    "    'random_forest': {\n",
    "        'model_type': 'rf',\n",
    "        'model_params': {\n",
    "            'n_estimators': 200,\n",
    "            'max_depth': 15,\n",
    "            'min_samples_split': 5,\n",
    "            'random_state': 42\n",
    "        }\n",
    "    },\n",
    "    'gradient_boost': {\n",
    "        'model_type': 'gb',\n",
    "        'model_params': {\n",
    "            'n_estimators': 150,\n",
    "            'learning_rate': 0.05,\n",
    "            'max_depth': 8,\n",
    "            'subsample': 0.8,\n",
    "            'random_state': 42\n",
    "        }\n",
    "    },\n",
    "    'xgboost': {\n",
    "        'model_type': 'xgb',\n",
    "        'model_params': {\n",
    "            'n_estimators': 150,\n",
    "            'learning_rate': 0.05,\n",
    "            'max_depth': 8,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'random_state': 42\n",
    "        }\n",
    "    },\n",
    "    'lightgbm': {\n",
    "        'model_type': 'lgbm',\n",
    "        'model_params': {\n",
    "            'n_estimators': 150,\n",
    "            'learning_rate': 0.05,\n",
    "            'num_leaves': 32,\n",
    "            'subsample': 0.8,\n",
    "            'random_state': 42\n",
    "        }\n",
    "    },\n",
    "    'catboost': {\n",
    "        'model_type': 'catboost',\n",
    "        'model_params': {\n",
    "            'iterations': 150,\n",
    "            'learning_rate': 0.05,\n",
    "            'depth': 8,\n",
    "            'random_state': 42,\n",
    "            'verbose': False\n",
    "        }\n",
    "    },\n",
    "    'extra_trees': {\n",
    "        'model_type': 'ext',\n",
    "        'model_params': {\n",
    "            'n_estimators': 200,\n",
    "            'max_depth': 15,\n",
    "            'min_samples_split': 5,\n",
    "            'random_state': 42\n",
    "        }\n",
    "    },\n",
    "    'neural_net': {\n",
    "        'model_type': 'mlp',\n",
    "        'model_params': {\n",
    "            'hidden_layer_sizes': (100, 50, 25),\n",
    "            'activation': 'relu',\n",
    "            'max_iter': 1000,  # Increased from 500\n",
    "            'early_stopping': True,  # Added early stopping\n",
    "            'random_state': 42\n",
    "        }\n",
    "    },\n",
    "    'robust_regression': {\n",
    "        'model_type': 'huber',\n",
    "        'model_params': {\n",
    "            'epsilon': 1.35,\n",
    "            'max_iter': 200,\n",
    "            'alpha': 0.0001\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size before split: 6583\n",
      "Training target stats:\n",
      "count    6.583000e+03\n",
      "mean     1.440515e+04\n",
      "std      9.878081e+04\n",
      "min      1.000000e-02\n",
      "25%      5.045000e+01\n",
      "50%      5.350000e+02\n",
      "75%      3.683100e+03\n",
      "max      4.779145e+06\n",
      "Name: total_fire_size, dtype: float64\n",
      "\n",
      "Dataset splits:\n",
      "Training samples: 5266\n",
      "Validation samples: 1317\n",
      "Test samples: 3000\n",
      "\n",
      "Feature statistics:\n",
      "       mean_elevation  Land Area (sq mi)  Water Area (sq mi)  \\\n",
      "count     3000.000000        3000.000000         3000.000000   \n",
      "mean       518.060000       70636.920000         5296.680000   \n",
      "std        521.427047       84967.355216        14115.137803   \n",
      "min         20.000000        1034.000000          192.000000   \n",
      "25%        180.000000       35826.000000          701.000000   \n",
      "50%        300.000000       53891.500000         1501.500000   \n",
      "75%        610.000000       81759.000000         4509.000000   \n",
      "max       2070.000000      570641.000000        94743.000000   \n",
      "\n",
      "       Percentage of Federal Land  Urbanization Rate (%)     latitude  \\\n",
      "count                 3000.000000            3000.000000  3000.000000   \n",
      "mean                     0.152560               0.724580    39.577038   \n",
      "std                      0.202186               0.141704     6.370963   \n",
      "min                      0.003000               0.351000    19.898682   \n",
      "25%                      0.019000               0.632000    35.517491   \n",
      "50%                      0.052000               0.721000    39.804187   \n",
      "75%                      0.202000               0.856000    43.299428   \n",
      "max                      0.801000               0.942000    63.588753   \n",
      "\n",
      "         longitude         PRCP         EVAP         TMIN         TMAX  \\\n",
      "count  3000.000000  3000.000000  3000.000000  3000.000000  3000.000000   \n",
      "mean    -93.665237    23.610548    64.783746     3.544597   300.569633   \n",
      "std      19.125998    17.809906   116.743240    85.838159    83.233947   \n",
      "min    -155.665857     0.000000   -96.870000  -333.000000   -83.000000   \n",
      "25%    -105.032363    10.783000    27.417317   -39.000000   272.000000   \n",
      "50%     -89.398528    22.210000    44.435000     0.000000   311.000000   \n",
      "75%     -79.019300    30.927961    63.130000    50.000000   350.000000   \n",
      "max     -69.445469   224.330000  2357.000000   228.000000   539.000000   \n",
      "\n",
      "       month_since_epoch  month_in_year  \n",
      "count        3000.000000    3000.000000  \n",
      "mean          521.500000       6.500000  \n",
      "std            17.320989       3.452628  \n",
      "min           492.000000       1.000000  \n",
      "25%           506.750000       3.750000  \n",
      "50%           521.500000       6.500000  \n",
      "75%           536.250000       9.250000  \n",
      "max           551.000000      12.000000  \n",
      "  State    month year_month  mean_elevation  Land Area (sq mi)  \\\n",
      "0    AK  2011-01    2011-01             580             570641   \n",
      "1    AK  2011-02    2011-02             580             570641   \n",
      "2    AK  2011-03    2011-03             580             570641   \n",
      "3    AK  2011-04    2011-04             580             570641   \n",
      "4    AK  2011-05    2011-05             580             570641   \n",
      "\n",
      "   Water Area (sq mi)  Percentage of Federal Land  Urbanization Rate (%)  \\\n",
      "0               94743                       0.609                  0.649   \n",
      "1               94743                       0.609                  0.649   \n",
      "2               94743                       0.609                  0.649   \n",
      "3               94743                       0.609                  0.649   \n",
      "4               94743                       0.609                  0.649   \n",
      "\n",
      "    latitude   longitude       PRCP       EVAP       TMIN        TMAX  \\\n",
      "0  63.588753 -154.493062  14.155368  27.475368 -16.042105  251.705263   \n",
      "1  63.588753 -154.493062  14.155368  27.475368 -16.042105  251.705263   \n",
      "2  63.588753 -154.493062   6.206316  38.111053 -25.789474  236.736842   \n",
      "3  63.588753 -154.493062   0.000000  48.000000  33.000000  122.000000   \n",
      "4  63.588753 -154.493062   2.970000  45.940000 -33.000000  294.000000   \n",
      "\n",
      "   month_since_epoch  month_in_year  season  \n",
      "0                492              1  winter  \n",
      "1                493              2  winter  \n",
      "2                494              3  spring  \n",
      "3                495              4  spring  \n",
      "4                496              5  spring  \n",
      "\n",
      "Training random_forest...\n",
      "\n",
      "Training gradient_boost...\n",
      "\n",
      "Training xgboost...\n",
      "\n",
      "Training lightgbm...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000297 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 985\n",
      "[LightGBM] [Info] Number of data points in the train set: 5266, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 14052.692419\n",
      "\n",
      "Training catboost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pawel4388/ML/hackathon/venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/pawel4388/ML/hackathon/venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training extra_trees...\n",
      "\n",
      "Training neural_net...\n",
      "\n",
      "Training robust_regression...\n",
      "\n",
      "Model Comparison (Custom Evaluation Metric):\n",
      "\n",
      "random_forest:\n",
      "Train error: 1.8065\n",
      "Validation error: 2.1777\n",
      "(Lower error is better)\n",
      "\n",
      "gradient_boost:\n",
      "Train error: 2.6208\n",
      "Validation error: 2.9330\n",
      "(Lower error is better)\n",
      "\n",
      "xgboost:\n",
      "Train error: 2.7008\n",
      "Validation error: 3.0123\n",
      "(Lower error is better)\n",
      "\n",
      "lightgbm:\n",
      "Train error: 3.5316\n",
      "Validation error: 3.5695\n",
      "(Lower error is better)\n",
      "\n",
      "catboost:\n",
      "Train error: 3.3360\n",
      "Validation error: 3.3624\n",
      "(Lower error is better)\n",
      "\n",
      "extra_trees:\n",
      "Train error: 1.6544\n",
      "Validation error: 2.0570\n",
      "(Lower error is better)\n",
      "\n",
      "neural_net:\n",
      "Train error: 3.2169\n",
      "Validation error: 3.2023\n",
      "(Lower error is better)\n",
      "\n",
      "robust_regression:\n",
      "Train error: 3.1334\n",
      "Validation error: 2.9362\n",
      "(Lower error is better)\n",
      "\n",
      "Using best model for predictions...\n",
      "Submission saved to submissions/submission.csv\n"
     ]
    }
   ],
   "source": [
    "# Initialize data processor\n",
    "data_processor = WildfireData(\n",
    "    fire_data_path='data/wildfire_sizes_before_2010.csv',\n",
    "    state_data_path='data/merged_state_data.csv',\n",
    "    weather_data_path='data/weather_monthly_state_aggregates.csv',\n",
    "    coordinates_path='data/state_coordinates.csv', \n",
    "    zero_submission_path='data/zero_submission.csv'\n",
    ")\n",
    "\n",
    "# Prepare data\n",
    "data_processor.prepare_data()\n",
    "\n",
    "X_test_original = data_processor.X_test.copy()\n",
    "print(X_test_original.head())\n",
    "\n",
    "data_processor.filter_features(['PRCP', 'EVAP', 'TMIN', 'TMAX', 'mean_elevation', \n",
    "                              'Land Area (sq mi)', 'Water Area (sq mi)', \n",
    "                              'Percentage of Federal Land', 'Urbanization Rate (%)', \n",
    "                              'latitude', 'longitude'])\n",
    "\n",
    "# Try all models\n",
    "results = {}\n",
    "best_model = None\n",
    "best_score = -np.inf\n",
    "feature_names = None\n",
    "\n",
    "for model_name, config in models_config.items():\n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    pipeline, train_score, val_score, feat_names = train_and_evaluate(data_processor, config)\n",
    "    feature_names = feat_names  # Save feature names for later use\n",
    "    \n",
    "    results[model_name] = {\n",
    "        'train_score': train_score,\n",
    "        'val_score': val_score,\n",
    "        'pipeline': pipeline\n",
    "    }\n",
    "    \n",
    "    if val_score > best_score:\n",
    "        best_score = val_score\n",
    "        best_model = pipeline\n",
    "\n",
    "# Print results with better formatting\n",
    "print(\"\\nModel Comparison (Custom Evaluation Metric):\")\n",
    "for model_name, scores in results.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"Train error: {-scores['train_score']:.4f}\")\n",
    "    print(f\"Validation error: {-scores['val_score']:.4f}\")\n",
    "    print(f\"(Lower error is better)\")\n",
    "\n",
    "# Use best model for predictions with numpy arrays\n",
    "print(f\"\\nUsing best model for predictions...\")\n",
    "X_test_transformed = best_model.named_steps['preprocessor'].transform(data_processor.X_test)\n",
    "y_pred = best_model.named_steps['model'].predict(X_test_transformed)\n",
    "\n",
    "# Create submission DataFrame with predictions\n",
    "submission_df = pd.DataFrame({\n",
    "    'STATE': X_test_original['State'],\n",
    "    'month': X_test_original['month'],\n",
    "    'total_fire_size': y_pred\n",
    "})\n",
    "\n",
    "create_submission(submission_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
